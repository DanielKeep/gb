/**
 * Implementation for gb.util.HashMap.
 *
 * Authors: Daniel Keep <daniel.keep@gmail.com>
 * Copyright: See LICENSE.
 */
module gb.util.impl.HashMap;

/* These control when the hashmap will grow or shrink.  These numbers are
 * talking about the ratio between the number of stored key/value pairs and
 * the total number of buckets.
 *
 * As a result, the hashmap will almost never be filled to capacity; keeping
 * some buckets free is good for performance.
 */

const LoadFactorMin = 0.25f;
const LoadFactorMax = 0.75f;

/* These functions are used to implement the sizing logic.  There are a number
 * of potential strategies one could use:
 *
 * - Multiples: if you need to grow/shrink the hashmap, you multiply/divide
 *   the number of buckets by some number (usually 2, 3 or 4).  Python uses a
 *   variant of this (it multiplies by two for small hashmaps, by four for
 *   large hashmaps).
 * 
 * - Polynomial: you resize the hashmap by taking the current number of
 *   buckets and substituting it into a polynomial expression.  For example,
 *   Java uses 2n+1.
 * 
 * - Primes: the number of buckets is selected from a static list of sizes;
 *   this is usually a sequence of prime numbers selected to be both distant
 *   from nearby powers of two and such that each successive prime is roughly
 *   the square of the previous one.  This is what D's builtin associative
 *   arrays use.
 *
 * Which strategy is best?  Hard to say.  If you have a perfect hash function,
 * then the strategy shouldn't matter.  Obviously, however, this is impossible
 * in practice.
 *
 * For now, we'll just stick to a constant multiplier.  However, this logic
 * has been broken out into functions so that it can be swapped out later.
 */

version( HashMap_Growth_Multiple2 )
    version = _HashMap_Growth_Multiple2;

else version( HashMap_Growth_SimplePoly )
    version = _HashMap_Growth_SimplePoly;

else
    version = _HashMap_Growth_Multiple2;

/* NOTE: All growBuckets implementations must return a positive, non-zero
 * result for growBuckets(0).  This is used when a HashMapHeader is being
 * initialised to create a minimally-sized map.
 *
 * shrinkBuckets should also have a sensible minimum result.
 */

version( _HashMap_Growth_Multiple2 )
{
    //
    // Simple multiple of two growth with base size of 8.
    //

    const MinimumBuckets = 8u;

    size_t growBuckets(size_t current)
    {
        auto newSize = 2*current;
        if( newSize < MinimumBuckets )
            return MinimumBuckets;
        if( newSize < current )
            return current;
        return newSize;
    }

    size_t shrinkBuckets(size_t current)
    {
        auto newSize = current / 2;
        if( newSize < MinimumBuckets )
            return MinimumBuckets;
        return newSize;
    }
}
else version( _HashMap_Growth_SimplePoly )
{
    //
    // Simple 2n+1 polynomial with 7 as the base size.
    //

    const MinimumBuckets = 7u;

    size_t growBuckets(size_t current)
    {
        auto newSize = 2*current + 1;
        if( newSize < MinimumBuckets )
            return MinimumBuckets;
        if( newSize < current )
            return current;
        return newSize;
    }

    size_t shrinkBuckets(size_t current)
    {
        auto newSize = (current-1) / 2;
        if( newSize < MinimumBuckets )
            return MinimumBuckets;
        return newSize;
    }
}

/**
 * Instances of this class are thrown for all exceptions generated by the
 * HashMap code.
 */
class HashMapException : Exception
{
    this(char[] msg)
    {
        super(msg);
    }

    static void throw_capacityOverflow()
    {
        throw_("cannot insert any more entries into hash map");
    }

    static void throw_removeMissing()
    {
        throw_("cannot remove value; could not find matching key");
    }

    static void throw_keyMissing()
    {
        throw_("cannot read from hash map; key not found");
    }

    private static void throw_(char[] msg)
    {
        throw new typeof(this)(msg);
    }
}

/**
 * This is the actual HashMap structure itself; HashMap(K,V) is just the
 * user-facing faÃ§ade.
 *
 * The HashMap is implemented using what I call (since no better name
 * immediately presented itself when I thought about it for all of four
 * seconds) "parallel index chaining".
 *
 * "Chaining" refers to the fairly standard practice of having the map's
 * buckets pointing to the head of some other structure (often a linked list).
 *
 * "Index" is used to indicate that the "nodes" in the chain list aren't
 * linked by pointers like with a normal linked list, but are instead linked
 * by indices into internally managed arrays.  This means that resizing the
 * map is probably more expensive; hopefully this is offset by hitting the GC
 * less often.
 *
 * "Parallel" is referring to how instead of using an actual node structure
 * which contains the key and value, we use a separate, parallel array for
 * each of the keys, values and next pointers.  This is to try and reduce the
 * chance of false pointers.
 *
 * The major impetuses (impeti?) for this design were:
 *
 * - Open hashing, where collisions are handled by searching in some pattern
 *   for another free bucket and then using that, turned out to be
 *   catastrophically awful for certain lookup patterns.  Checking for a
 *   non-stored key in a nearly full open hashing map would approach O(n)
 *   complexity.
 *
 * - Chaining with a singly linked list was the simplest alternative I could
 *   think of: the implementation merely needs to be *good enough* to replace
 *   the builtin associative arrays.
 *
 * - Using indices instead of pointers makes resizing and copying the map
 *   significantly simpler.  If I had used pointers instead, the entire map
 *   would have needed to be "fixed" every time the it was resized or a copy
 *   was made.  With indices, I need only preserve the ordering of the
 *   entries.
 *
 * - Parallel arrays gives the GC the best possible chance, via its primitive
 *   (has pointers/doesn't have pointers) flag, to avoid the spurious false
 *   pointer problems that can literally kill long-running programs that use
 *   builtin AAs.
 *
 * Some other notes:
 *
 * Why don't you cache the hash?
 *      Thus far, benchmarking has shown that caching the hash has a
 *      *negative* impact on performance.  There is most likely a key size
 *      above which caching is more efficient, but I haven't determined it
 *      yet.
 */
struct HashMapHeader(Key, Value, Hash)
{
    /**
     * index is used as a bit of a hack to trick the GC into initialising
     * arrays of indices to ~0 instead of zero; zero is an actual, valid value
     * to have, so we want the arrays initialised to something else.
     *
     * Heaven forbid we should be able to, oh I don't know, tell the GC what
     * to initialise allocations with.  ...  Nah; that'd never be useful.
     */
    private typedef size_t index = ~0;

    size_t  length  = 0,    /// The size of the buffers
            entries = 0;    /// Number of elements being stored
    index*  buckets = null; /// Buckets contain indices into the other buffers
    Key*    keys    = null; /// Key of entry
    Value*  values  = null; /// Value of entry
    index*  next    = null; /// Index of entry for chaining

    /**
     * Creates a new, empty header.  Note that a defaultly-initialised
     * HashMapHeader may *not* be valid for any operation; it certainly hasn't
     * been tested.
     *
     * Always create your maps before using them!
     */
    static HashMapHeader* create()
    {
        auto r = new HashMapHeader;
        auto l = r.length = growBuckets(0);
        allocStorage(l, r.buckets, r.keys, r.values, r.next);
        return r;
    }

    /**
     * Clears out the entire contents of the hashmap; this de-allocates
     * everything.
     */
    void clear()
    {
        length = entries = 0;
        freeStorage(buckets, keys, values, next);
    }

    /**
     * Copies the contents of this header into another one.  Note that this
     * *does not* deallocate anything on the destination; it should only be
     * passed an "unused" header as the destination.
     */
    void copyTo(inout HashMapHeader dst)
    {
        auto l = length;

        dst.length  = l;
        dst.entries = this.entries;

        allocStorage(l, dst.buckets, dst.keys, dst.values, dst.next);

        dst.buckets[0..l]   = this.buckets[0..l];
        dst.keys[0..l]      = this.keys[0..l];
        dst.values[0..l]    = this.values[0..l];
        dst.next[0..l]      = this.next[0..l];
    }

    /**
     * Inserts a key,value pair into the map.  If the key is already present
     * in the map, it will overwrite the old value.
     */
    void insert(ref Key key, ref Value value)
    {
        if( loadFactor(entries+1) > LoadFactorMax )
            grow();

        if( entries == length )
            // Oh dear...
            HashMapException.throw_capacityOverflow();

        // Get the hash and bucket
        auto keyHash = computeHash(key);
        auto bucket = keyHash % length;

        // Find out where to attach the new entry.  It will either be the
        // bucket itself or on the next pointer for the last entry chained
        // from that bucket.
        index* entryIndex;
        
        if( buckets[bucket] == index.init )
        {
            entryIndex = &buckets[bucket];
        }
        else
        {
            index cur = buckets[bucket];
            while( next[cur] != index.init )
                cur = next[cur];
            entryIndex = &next[cur];
        }

        // Do the insert.
        *entryIndex     = cast(index) entries;
        keys[entries]   = key;
        values[entries] = value;
        ++ entries;
    }

    /**
     * Attempts to retrieve the value associated with the given key from the
     * map.  If the value was found, it will return a pointer to its storage.
     * If the key isn't defined in the map, it will return null.
     *
     * The pointer should be used and discarded *immediately* as the pointer
     * can potentially be modified by future insert or remove operations.
     */
    Value* lookup(ref Key key)
    {
        if( entries == 0 )
            return null;

        auto keyHash = computeHash(key);
        auto bucket = keyHash % length;

        index entry = buckets[bucket];

        if( entry == index.init )
            // Nothing IN the bucket, so it clearly isn't in the map.
            return null;

        do
        {
            if( keys[entry] == key )
                return &values[entry];

            entry = next[entry];
        }
        while( entry != index.init );

        return null;
    }

    /**
     * Attempts to remove the specified key from the map.  Returns true if it
     * was removed, false if the key wasn't present.
     *
     * Remove can cause the map to shrink.  It may also cause the last
     * inserted value's physical address to change.
     */
    bool remove(ref Key key)
    {
        if( entries == 0 )
            return false;

        auto keyHash = computeHash(key);
        auto bucket = keyHash % length;

        index entry = buckets[bucket];

        if( entry == index.init )
            // Nothing in the bucket, so we obviously can't remove it
            return false;

        // remEntryPtr will eventually contain a pointer to where the index to
        // the entry we're removing is being stored.  This will be used to
        // maintain the singly linked list.
        //
        // remEntry will be the index of the entry we want to remove in the
        // parallel buffers.
        index* remEntryPtr = &buckets[bucket];
        index remEntry;

        // Starting with the index in the bucket array, walk the list.
        do
        {
            if( keys[entry] == key )
            {
                remEntry = entry;
                break;
            }

            remEntryPtr = &next[entry];
            entry = next[entry];
        }
        while( entry != index.init );

        // If the entry to remove is physically last in the buffers, this is a
        // piece of cake.
        if( remEntry == entries-1 )
        {
            -- entries;
            *remEntryPtr    = index.init;
            keys[entries]   = Key.init;
            values[entries] = Value.init;
            next[entries]   = index.init;

            // What's the worst that could ha--RAPTOR ATTACK!
            goto handleShrink;
        }

        // Join the linked list back together.
        *remEntryPtr = next[remEntry];

        // To remove the indicated entry, we'll copy the last entry in the
        // map over to the top of the entry we're removing.  After which, we
        // need to fix up the next index for whatever was pointing at the
        // entry we moved.
        -- entries;
        keys[remEntry]      = keys[entries];
        values[remEntry]    = values[entries];
        next[remEntry]      = next[entries];

        keys[entries]       = Key.init;
        values[entries]     = Value.init;
        next[entries]       = index.init;

        // swp* = swapped element
        auto swpHash = computeHash(keys[remEntry]);
        auto swpBucket = swpHash % length;

        index swpEntry = buckets[swpBucket];

        // Did the bucket point directly to the old entry?
        if( swpEntry == entries )
        {
            buckets[swpBucket] = remEntry;
        }
        else
        {
            do
            {
                assert( swpBucket != index.init );

                if( next[swpEntry] == entries )
                {
                    next[swpEntry] = remEntry;
                    break;
                }

                swpEntry = next[swpEntry];
            }
            while( true );
        }

        // OK; we've removed the entry, shuffled the last down into its
        // place and fixed up the pointer to the moved entry.
        // 
        // The last thing we need to worry about is shrinking the map if we
        // fall below the minimum load factor.
        
handleShrink:
        if( loadFactor() < LoadFactorMin )
            shrink();

        return true;
    }

    /*

       Methods needed for the implementation of the public wrapper.

    */

    Key[] keysToArray()
    {
        if( entries == 0 )
            return null;

        auto arr = new Key[](entries);
        arr[] = keys[0..entries];

        return arr;
    }

    Value[] valuesToArray()
    {
        if( entries == 0 )
            return null;

        auto arr = new Value[](entries);
        arr[] = values[0..entries];

        return arr;
    }

    int iterKeys(int delegate(ref Key) dg)
    {
        int r = 0;
        Key k;
        for( size_t i=0; i<entries; ++i )
        {
            k = keys[i];
            r = dg(k);
            if( r ) break;
        }
        return r;
    }

    int iterValues(int delegate(ref Value) dg)
    {
        int r = 0;
        auto valarr = values[0..entries];
        for( size_t i=0; i<entries; ++i )
        {
            r = dg(valarr[i]);
            if( r ) break;
        }
        return r;
    }

    int iterItems(int delegate(ref Key, ref Value) dg)
    {
        int r = 0;
        Key k;
        auto valarr = values[0..entries];
        for( size_t i=0; i<entries; ++i )
        {
            k = keys[i];
            r = dg(k, valarr[i]);
            if( r ) break;
        }
        return r;
    }

private:
    /**
     * Computes the load factor of the map; that is the ratio of entries to
     * the total number of available slots.
     *
     * The overload with one argument is used to compute the load factor for a
     * specified number of entries instead of the number of actual entries in
     * the map right now.
     *
     * This is used to decide when to resize the map.
     */
    float loadFactor()
    {
        return loadFactor(entries);
    }

    /// ditto
    float loadFactor(size_t entries)
    {
        return (cast(float) entries) / length;
    }

    /**
     * Computes the hash of the given key.
     */
    static hash_t computeHash(ref Key key)
    {
        // Historical note: originally used to prevent us from ever getting a
        // hash of 0, which indicated an empty bucket for open hashing.

        /* We'll special case a few really common key types that don't have a
         * meaningful hash function.
         */
        static if( is( Key : long ) && Key.sizeof <= hash_t.sizeof )
        {
            return cast(hash_t) key;
        }
        else
        {
            return typeid(Key).getHash(&key);
        }
    }

    /**
     * Tries to grow the size of the map.  It may not actually do anything,
     * which is OK.  One should always manually ensure there is room to insert
     * something even after calling this method.
     */
    void grow()
    {
        // First, determine if we CAN actually grow.
        auto newLength = growBuckets(length);
        if( newLength == length )
            // Guess not.
            return;

        // Allons-ye!
        reallocAndReinsert(newLength);
    }

    /**
     * Tries to shrink the map's storage.
     */
    void shrink()
    {
        // Find out what we'll shrink to.
        auto newLength = shrinkBuckets(length);
        
        // There are a few reasons why we might abort at this point:
        //
        // 1. The new length isn't any different (we're at the smallest bucket
        //    size).
        // 
        // 2. The new length is too small to contain all the entries.  This
        //    could theoretically happen with very high minimum load factors.
        //
        // 3. The new length would lead to a load factor ABOVE the maximum
        //    load factor.  We want to avoid this.

        if( newLength == length
                || newLength < entries
                || ((cast(float) entries) / newLength) > LoadFactorMax )
            return;

        // Otherwise, we'll proceed.
        reallocAndReinsert(newLength);
        return;
    }

    /**
     * Reallocates the map's arrays with a new length and migrates the
     * existing data.  It then replaces and deletes the old storage.
     */
    void reallocAndReinsert(size_t newLength)
    {
        // Allocate the new storage.
        index*  newBuckets;
        Key*    newKeys;
        Value*  newValues;
        index*  newNext;
        allocStorage(newLength, newBuckets, newKeys, newValues, newNext);

        // Copy the keys and values.
        newKeys[0..entries]   = keys[0..entries];
        newValues[0..entries] = values[0..entries];

        // Deallocate old arrays and swap in the new ones.
        freeStorage(buckets, keys, values, next);

        length  = newLength;
        buckets = newBuckets;
        keys    = newKeys;
        values  = newValues;
        next    = newNext;

        // Rebuild the buckets and links
        for( size_t entry=0; entry<entries; ++entry )
        {
            auto keyHash = computeHash(keys[entry]);
            auto bucket = keyHash % length;

            index* entryIndex;

            if( buckets[bucket] == index.init )
            {
                entryIndex = &buckets[bucket];
            }
            else
            {
                index cur = buckets[bucket];
                while( next[cur] != index.init )
                    cur = next[cur];
                entryIndex = &next[cur];
            }

            assert( entryIndex !is null );

            *entryIndex = cast(index) entry;
        }
    }

    /**
     * Allocates storage for the given size.  Should only be paired with a
     * call to freeStorage as it may have done... "things" to the
     * allocations.  Terrible, unspeakable THINGS.
     */
    static void allocStorage(size_t length,
            ref index* buckets,
            ref Key* keys,
            ref Value* values,
            ref index* next)
    {
        auto indices = (new index[](2*length)).ptr;

        buckets = indices;
        keys    = (new Key[](length)).ptr;
        values  = (new Value[](length)).ptr;
        next    = indices + length;
    }

    /**
     * Frees some storages allocated by allocStorage.  Will also null out the
     * pointers for you.
     *
     * Because it's a people-perso, er, -function.
     */
    static void freeStorage(
            ref index* buckets,
            ref Key* keys,
            ref Value* values,
            ref index* next)
    {
        // Null next NOW since we don't want a dangling pointer left when we
        // delete buckets.
        next = null;

        delete buckets;
        delete keys;
        delete values;
    }
}

